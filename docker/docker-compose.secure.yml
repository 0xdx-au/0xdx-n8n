# 2025-07-20 CH41B01
# Secure N8N deployment with Caddy reverse proxy, HTTPS, and network isolation

version: '3.8'

services:
  # Caddy reverse proxy with automatic HTTPS
  caddy:
    image: caddy:2-alpine
    container_name: n8n-caddy-secure
    restart: unless-stopped
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./security/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy-data:/data
      - caddy-config:/config
    networks:
      - n8n-secure-network
    environment:
      - CADDY_INGRESS_NETWORKS=n8n-secure-network
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # N8N application with hardened security
  n8n:
    build:
      context: .
      dockerfile: Dockerfile.secure
    container_name: n8n-app-secure
    restart: unless-stopped
    expose:
      - "5678"
    environment:
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY:-n8n-super-secure-key-change-me-in-production-123456789}
      - N8N_USER_FOLDER=/home/node/.n8n
      - N8N_LOG_LEVEL=warn
      - N8N_LOG_OUTPUT=file
      - N8N_LOG_FILE_LOCATION=/var/log/n8n/n8n.log
      - N8N_SECURE_COOKIE=true
      - N8N_JWT_AUTH_HEADER=true
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-SecureP@ss123}
      - N8N_DISABLE_UI=${N8N_DISABLE_UI:-false}
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_VERSION_NOTIFICATIONS_ENABLED=false
      - N8N_TEMPLATES_ENABLED=true
      - N8N_PERSONALIZATION_ENABLED=false
      - N8N_HIRING_BANNER_ENABLED=false
      - WEBHOOK_URL=https://localhost
      - NODE_ENV=production
      - TZ=UTC
    volumes:
      - n8n-data:/home/node/.n8n:rw
      - n8n-logs:/var/log/n8n:rw
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - n8n-secure-network
    depends_on:
      - n8n-db
    user: "node"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/cache:noexec,nosuid,size=50m
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # PostgreSQL database with security hardening
  n8n-db:
    image: postgres:16-alpine
    container_name: n8n-database-secure
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-n8n}
      - POSTGRES_USER=${POSTGRES_USER:-n8n_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-SecureDB_P@ss123}
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256 --auth-local=scram-sha-256
    volumes:
      - postgres-data:/var/lib/postgresql/data:rw
      - ./security/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    networks:
      - n8n-secure-network
    expose:
      - "5432"
    user: "postgres"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-n8n_user} -d ${POSTGRES_DB:-n8n}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Prometheus for monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: n8n-prometheus-secure
    restart: unless-stopped
    expose:
      - "9090"
    volumes:
      - ./security/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus:rw
    networks:
      - n8n-secure-network
      - monitoring-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    user: "nobody"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana for dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: n8n-grafana-secure
    restart: unless-stopped
    expose:
      - "3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-SecureGrafana123!}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_INSTALL_PLUGINS=
      - GF_SERVER_PROTOCOL=http
      - GF_SERVER_DOMAIN=localhost
      - GF_SERVER_ROOT_URL=https://localhost/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
    volumes:
      - grafana-data:/var/lib/grafana:rw
      - ./security/grafana.ini:/etc/grafana/grafana.ini:ro
    networks:
      - monitoring-network
    user: "472"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # OWASP ZAP for security scanning
  zap:
    image: zaproxy/zap-stable:latest
    container_name: n8n-zap-scanner
    restart: "no"
    expose:
      - "8080"
    volumes:
      - zap-data:/zap/wrk:rw
    networks:
      - n8n-secure-network
    command: zap-baseline.py -t https://caddy -J /zap/wrk/zap-report.json -r /zap/wrk/zap-report.html -I
    depends_on:
      - caddy
      - n8n
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL

  # Fail2Ban for intrusion prevention
  fail2ban:
    image: crazymax/fail2ban:latest
    container_name: n8n-fail2ban
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - fail2ban-data:/data:rw
      - ./security/fail2ban:/etc/fail2ban:ro
    environment:
      - TZ=UTC
      - F2B_LOG_LEVEL=INFO
      - F2B_DB_PURGE_AGE=1d
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    security_opt:
      - no-new-privileges:true

volumes:
  # Persistent data with security labels
  caddy-data:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid,nodev
      device: ${PWD}/volumes/caddy/data
  caddy-config:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid,nodev
      device: ${PWD}/volumes/caddy/config
  n8n-data:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid
      device: ${PWD}/volumes/n8n/data
  n8n-logs:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid,nodev
      device: ${PWD}/volumes/n8n/logs
  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid,nodev
      device: ${PWD}/volumes/postgres/data
  prometheus-data:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid,nodev
      device: ${PWD}/volumes/prometheus/data
  grafana-data:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid,nodev
      device: ${PWD}/volumes/grafana/data
  zap-data:
    driver: local
    driver_opts:
      type: none
      o: bind,noexec,nosuid,nodev
      device: ${PWD}/volumes/zap/data
  fail2ban-data:
    driver: local

networks:
  # Isolated networks with security policies
  n8n-secure-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: n8n-secure-br
      com.docker.network.bridge.enable_icc: "false"
      com.docker.network.bridge.enable_ip_masquerade: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          ip_range: 172.20.240.0/20
          gateway: 172.20.0.1
    labels:
      - "com.0xdx.network.security=high"
      - "com.0xdx.network.purpose=n8n-application"
      
  monitoring-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: n8n-monitor-br
      com.docker.network.bridge.enable_icc: "false"
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
          ip_range: 172.21.240.0/20
          gateway: 172.21.0.1
    labels:
      - "com.0xdx.network.security=high"
      - "com.0xdx.network.purpose=monitoring"
